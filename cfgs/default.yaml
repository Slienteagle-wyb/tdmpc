# environment
task: quadruped-run
modality: 'state'
action_repeat: ???
discount: 0.99
episode_length: 1000/${action_repeat}
train_steps: 500000/${action_repeat}

# planning
iterations: 6
num_samples: 512
num_elites: 64
mixture_coef: 0.5  # 0.05
min_std: 0.05
temperature: 0.5
momentum: 0.1

# learning
batch_size: 512
max_buffer_size: 1000000
horizon: 5
reward_coef: 0.5
value_coef: 0.1
consistency_coef: 2
similarity_coef: 1.0
intrinsic_reward_coef: 0.5
rho: 0.5
kappa: 0.1
std_schedule: linear(0.5, ${min_std}, 25000, 0)  # duration 25000 train step
horizon_schedule: linear(1, ${horizon}, 25000, 0)
explore_schedule: linear(0, ${intrinsic_reward_coef}, 25000, 5000)
regularization_schedule: linear(0.05, ${mixture_coef}, 1, 5000)
per_alpha: 0.6
per_beta: 0.4
grad_clip_norm: 10
seed_steps: 5000
update_freq: 2
tau: 0.01

# optim
lr: 1e-3
pi_lr: 1e-3
optim_id: 'adamw'
weight_decay: 0.0
#sched_kwargs:
#  sched_id: 'cosine'
#  min_lr: 1e-6
#  warmup_lr_init: 1e-5
#  warmup_epochs: 20

# architecture
enc_dim: 256
mlp_dim: 512
latent_dim: 128

# wandb (insert your own)
use_wandb: ture
wandb_project: MoPAC
wandb_entity: Slientea98
wandb_exp_name: 1130_cheetah_run_similarity_explore0.5_sched200k_mix0.5_delay

# misc
seed: 8
env_horizon: 0  # 0 if normal transition
dream_horizon : 1
dream_trace: 8
exp_name: default
eval_freq: 20000
eval_episodes: 10
save_video: false
save_model: false
device: 'cuda'
normalize: true
norm_type: 'ln'  # 'bn' for quadruped

# sac
q_lr: 1e-3
gamma: 0.99
sac_tau: 0.005
latent_policy: false